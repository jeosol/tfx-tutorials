{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_tuner_study.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOco0wpqfP53f8clMkEBuV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeosol/tfx-tutorials/blob/main/keras_tuner_study.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras Tuner (from keras.io/keras_tuner) \n",
        "\n",
        "KerasTuner is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search. Easily configure you search space with define-by-run syntax, then leverage one of the available search algorithms to find the best hyperparameter values for your models. KerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms built-in, and is also designed to be easy for researchers to extend in order to experiment with new search algorithms."
      ],
      "metadata": {
        "id": "VeDx8LSgwJmj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Otl4sCUdmzcx",
        "outputId": "88c75548-23b5-4601-d3f0-c1204454e92c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██▍                             | 10 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20 kB 37.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 30 kB 43.5 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 40 kB 44.7 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 51 kB 47.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 61 kB 52.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 71 kB 32.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 81 kB 32.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 92 kB 35.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 102 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 112 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 122 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 133 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 135 kB 36.5 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "YpfSHx3Jm56X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kerastuner as kt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XEMFP3M4nByK",
        "outputId": "849c4e68-4fb4-45c7-bae1-2cf042dfd9d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Several options for constructing the mode and using kerastuner\n",
        "# sources: keras.io getting_started guide \n",
        "\n",
        "# if we use a simple build_model function\n",
        "def build_model(hp):\n",
        "  units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "  model = keras.Sequential([\n",
        "    layers.Dense(units, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "  ])\n",
        "  optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "  model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "  return model\n",
        "\n",
        "# build_model(keras_tuner.Hyperparameters())\n",
        "# \n",
        "\n",
        "\n",
        "class SimpleMLP(kt.HyperModel):\n",
        "  def __init__(self, num_classes):\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  # Build function takes a hp object\n",
        "  def build(self, hp):\n",
        "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(units, activation=\"relu\"),\n",
        "        layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    # we can try two choices for the optimizer\n",
        "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model \n",
        "\n",
        "# Option2: You can define the hyperparameters in advance and have the code\n",
        "# constructing the model as separate\n",
        "\n",
        "# this function takes the possible hyperparameterrs\n",
        "def call_existing_code(units, activation, dropout, lr):\n",
        "  model = keras.Sequential()\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(units=units, activation=activation))\n",
        "  if dropout:\n",
        "    model.add(layers.Dropout(rate=0.25))\n",
        "  model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      metrics=[\"accuracy\"]\n",
        "  )\n",
        "  return model\n",
        "\n",
        "def build_modelv2(hp): \n",
        "  units = hp.Int(\"units\", min_values=32, max_value=512, step=32)\n",
        "  activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "  dropout = hp.Boolean(\"dropout\")\n",
        "  lr = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "  # call existing model-building code with the hyperparameter values\n",
        "  model = call_existing_code(\n",
        "      units=units, activation=activation, dropout=dropout, lr=lr\n",
        "  )\n",
        "  return model \n",
        "\n",
        "class SimpleMLPv2(kt.HyperModel):\n",
        "  def __init__(self, num_classes):\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  # Build function takes a hp object\n",
        "  def build(self, hp):    \n",
        "    model = keras.Sequential()\n",
        "    model.add(\n",
        "        layers.Dense(\n",
        "            # Tune the number of units\n",
        "            units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        )\n",
        "    )\n",
        "    # Tune whether to use dropout\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "      model.add(layers.Dropout(rate=0.25))\n",
        "\n",
        "    # add the classification layer                   \n",
        "    model.add(layers.Dense(self.num_classes, activation=\"softmax\"))\n",
        "    \n",
        "    # define the optimizer learning rate as a hyperparameter\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    \n",
        "    # we can try two choices for the optimizer\n",
        "    #optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model \n",
        "\n",
        "# Example which tunes the number of layers\n",
        "class SimpleMLPv3(kt.HyperModel):\n",
        "  def __init__(self, num_classes):\n",
        "    self.num_classes = num_classes\n",
        "\n",
        "  # Build function takes a hp object\n",
        "  def build(self, hp):    \n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Flatten())\n",
        "    # Tune the number of layrers\n",
        "    for i in range(hp.Int(\"num_layers\", 1, 3)):      \n",
        "      model.add(\n",
        "        layers.Dense(\n",
        "            # Tune the number of units\n",
        "            units = hp.Int(name=f\"units_{i}\", min_value=16, max_value=64, step=16),\n",
        "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"])\n",
        "        )\n",
        "      )\n",
        "    # Tune whether to use dropout\n",
        "    if hp.Boolean(\"dropout\"):\n",
        "      model.add(layers.Dropout(rate=0.25))\n",
        "\n",
        "    # add the classification layer                   \n",
        "    model.add(layers.Dense(self.num_classes, activation=\"softmax\"))\n",
        "    \n",
        "    # define the optimizer learning rate as a hyperparameter\n",
        "    learning_rate = hp.Float(\"lr\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
        "    \n",
        "    # we can try two choices for the optimizer\n",
        "    #optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model \n",
        "    "
      ],
      "metadata": {
        "id": "TRJVjHA0nNSq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypermodel = SimpleMLP(num_classes=10)"
      ],
      "metadata": {
        "id": "J9AndVmUnwbN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we pick a tuner. \n",
        "tuner = kt.BayesianOptimization(\n",
        "   hypermodel,\n",
        "   objective=\"val_accuracy\", # the parameter that tuner will seek to optimize\n",
        "   max_trials=10, # maximum number of different model configuration trials ,\n",
        "   executions_per_trial=2,\n",
        "   directory=\"mnist_kt_test\",\n",
        "   project_name=\"mnist_hp\",\n",
        "   overwrite=True,\n",
        ")"
      ],
      "metadata": {
        "id": "KSZMulsMn1oT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display an overview of the search space via search_space_summary()\n",
        "tuner.search_space_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaWJamaXoLj0",
        "outputId": "7b7ca322-3a39-49e8-880b-be93bbf0e412"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search space summary\n",
            "Default search space size: 2\n",
            "units (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': None}\n",
            "optimizer (Choice)\n",
            "{'default': 'rmsprop', 'conditions': [], 'values': ['rmsprop', 'adam'], 'ordered': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "x_test  = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "\n",
        "# reserve the full datasets for later\n",
        "x_train_full = x_train[:]\n",
        "y_train_full = y_train[:]"
      ],
      "metadata": {
        "id": "6j52UpZcoect"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_val_samples = 10000\n",
        "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
        "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
        "\n",
        "# add some callbacks\n",
        "# patience is the number of epochs with no improvement after which training\n",
        "# will be stopped.\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
        "]\n",
        "\n",
        "# Use a large number of epochs (you don't know in advance how many epocs \n",
        "# each model will need), and use an EarlyStopping callback to stop training\n",
        "# when you start overfitting\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-3gPqSApBuo",
        "outputId": "0b9ba6cd-5276-44af-e593-2a19b981c960"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 39s]\n",
            "val_accuracy: 0.9733499884605408\n",
            "\n",
            "Best val_accuracy So Far: 0.9754500091075897\n",
            "Total elapsed time: 00h 10m 26s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 4\n",
        "best_hps = tuner.get_best_hyperparameters(top_n)"
      ],
      "metadata": {
        "id": "XDv6BBpNqNAh"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# in callback Earlystopping\n",
        "# mode='min', training will stop once the quantity monitored has stopped decreasing\n",
        "# mode='max', training will stop once the quantity monitored has stopped increasing\n",
        "# mode='auto', the direction is automatically inferred from the name of the monitored quantity\n",
        "\n",
        "# use the validation set to find the best epochs\n",
        "def get_best_epoch(hp):\n",
        "  model = hypermodel.build(hp)\n",
        "  callbacks=[\n",
        "      keras.callbacks.EarlyStopping(\n",
        "          monitor=\"val_loss\", mode=\"min\", patience=10),\n",
        "  ]\n",
        "  history = model.fit(\n",
        "      x_train, y_train,\n",
        "      validation_data=(x_val, y_val),\n",
        "      epochs=100,\n",
        "      batch_size=128,\n",
        "      callbacks=callbacks\n",
        "  )\n",
        "  val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "  # add 1 because index is zero-based.\n",
        "  best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "  print(\"Best epoch: {}\".format(best_epoch))\n",
        "  return best_epoch, model\n"
      ],
      "metadata": {
        "id": "d6ONkDd2qZJ5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally, train on the full dataset for just a bit longer than this epoch count,\n",
        "# since you are training on more data; 20% more in this case\n",
        "\n",
        "def get_best_trained_model(hp):\n",
        "  best_epoch, model = get_best_epoch(hp) \n",
        "  model.fit(\n",
        "      x_train_full, y_train_full, \n",
        "      batch_size=128, epochs=int(best_epoch * 1.2))\n",
        "  return model \n",
        "\n",
        "best_models = []\n",
        "\n",
        "for hp in best_hps: \n",
        "  # get the hyperparameter and train the model on the full dataset\n",
        "  model = get_best_trained_model(hp) \n",
        "  # evaluate the model on the held-out test dataset\n",
        "  model.evaluate(x_test, y_test)\n",
        "  # save the model in the list of best models\n",
        "  best_models.append(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFKOwfX7t_bp",
        "outputId": "0a2fd84d-971b-43d1-a79c-ac21b879bc96"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "391/391 [==============================] - 2s 3ms/step - loss: 0.4194 - accuracy: 0.8878 - val_loss: 0.2355 - val_accuracy: 0.9338\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.2196 - accuracy: 0.9371 - val_loss: 0.1802 - val_accuracy: 0.9490\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1703 - accuracy: 0.9501 - val_loss: 0.1533 - val_accuracy: 0.9571\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1407 - accuracy: 0.9592 - val_loss: 0.1356 - val_accuracy: 0.9621\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1199 - accuracy: 0.9654 - val_loss: 0.1292 - val_accuracy: 0.9625\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1043 - accuracy: 0.9703 - val_loss: 0.1145 - val_accuracy: 0.9673\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0924 - accuracy: 0.9726 - val_loss: 0.1191 - val_accuracy: 0.9658\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0828 - accuracy: 0.9757 - val_loss: 0.1081 - val_accuracy: 0.9698\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0741 - accuracy: 0.9788 - val_loss: 0.1045 - val_accuracy: 0.9704\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0674 - accuracy: 0.9803 - val_loss: 0.0982 - val_accuracy: 0.9726\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0615 - accuracy: 0.9821 - val_loss: 0.1059 - val_accuracy: 0.9712\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9839 - val_loss: 0.0942 - val_accuracy: 0.9739\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9859 - val_loss: 0.0921 - val_accuracy: 0.9748\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0477 - accuracy: 0.9864 - val_loss: 0.1031 - val_accuracy: 0.9703\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0436 - accuracy: 0.9876 - val_loss: 0.0947 - val_accuracy: 0.9735\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0407 - accuracy: 0.9887 - val_loss: 0.0959 - val_accuracy: 0.9739\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0376 - accuracy: 0.9901 - val_loss: 0.1025 - val_accuracy: 0.9726\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0344 - accuracy: 0.9908 - val_loss: 0.0959 - val_accuracy: 0.9743\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0322 - accuracy: 0.9914 - val_loss: 0.0980 - val_accuracy: 0.9740\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9920 - val_loss: 0.0970 - val_accuracy: 0.9753\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0273 - accuracy: 0.9929 - val_loss: 0.0985 - val_accuracy: 0.9746\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9933 - val_loss: 0.0979 - val_accuracy: 0.9755\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0233 - accuracy: 0.9940 - val_loss: 0.0993 - val_accuracy: 0.9741\n",
            "Best epoch: 13\n",
            "Epoch 1/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0356 - accuracy: 0.9908\n",
            "Epoch 2/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0317 - accuracy: 0.9915\n",
            "Epoch 3/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0286 - accuracy: 0.9925\n",
            "Epoch 4/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9928\n",
            "Epoch 5/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0241 - accuracy: 0.9933\n",
            "Epoch 6/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0223 - accuracy: 0.9940\n",
            "Epoch 7/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0208 - accuracy: 0.9945\n",
            "Epoch 8/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0191 - accuracy: 0.9950\n",
            "Epoch 9/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0179 - accuracy: 0.9954\n",
            "Epoch 10/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0164 - accuracy: 0.9958\n",
            "Epoch 11/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0150 - accuracy: 0.9963\n",
            "Epoch 12/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0142 - accuracy: 0.9967\n",
            "Epoch 13/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0136 - accuracy: 0.9966\n",
            "Epoch 14/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 0.9972\n",
            "Epoch 15/15\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0112 - accuracy: 0.9973\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.9757\n",
            "Epoch 1/100\n",
            "391/391 [==============================] - 2s 3ms/step - loss: 0.4648 - accuracy: 0.8749 - val_loss: 0.2352 - val_accuracy: 0.9331\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.2189 - accuracy: 0.9380 - val_loss: 0.1818 - val_accuracy: 0.9507\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1712 - accuracy: 0.9517 - val_loss: 0.1549 - val_accuracy: 0.9578\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.1408 - accuracy: 0.9595 - val_loss: 0.1326 - val_accuracy: 0.9618\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1200 - accuracy: 0.9658 - val_loss: 0.1240 - val_accuracy: 0.9634\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1035 - accuracy: 0.9709 - val_loss: 0.1163 - val_accuracy: 0.9659\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0912 - accuracy: 0.9736 - val_loss: 0.1080 - val_accuracy: 0.9690\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0807 - accuracy: 0.9771 - val_loss: 0.1037 - val_accuracy: 0.9693\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0721 - accuracy: 0.9797 - val_loss: 0.1015 - val_accuracy: 0.9707\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0643 - accuracy: 0.9819 - val_loss: 0.1029 - val_accuracy: 0.9702\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0580 - accuracy: 0.9838 - val_loss: 0.0965 - val_accuracy: 0.9719\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0529 - accuracy: 0.9850 - val_loss: 0.0922 - val_accuracy: 0.9723\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0477 - accuracy: 0.9868 - val_loss: 0.0932 - val_accuracy: 0.9736\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0429 - accuracy: 0.9884 - val_loss: 0.0971 - val_accuracy: 0.9696\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0396 - accuracy: 0.9889 - val_loss: 0.0964 - val_accuracy: 0.9715\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.0358 - accuracy: 0.9903 - val_loss: 0.0909 - val_accuracy: 0.9736\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 0.0915 - val_accuracy: 0.9744\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0292 - accuracy: 0.9922 - val_loss: 0.0945 - val_accuracy: 0.9730\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0263 - accuracy: 0.9935 - val_loss: 0.0965 - val_accuracy: 0.9720\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0236 - accuracy: 0.9946 - val_loss: 0.0963 - val_accuracy: 0.9729\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9947 - val_loss: 0.0976 - val_accuracy: 0.9727\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.0943 - val_accuracy: 0.9726\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0180 - accuracy: 0.9961 - val_loss: 0.0954 - val_accuracy: 0.9729\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0158 - accuracy: 0.9964 - val_loss: 0.0958 - val_accuracy: 0.9739\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0143 - accuracy: 0.9972 - val_loss: 0.1019 - val_accuracy: 0.9723\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 0.1049 - val_accuracy: 0.9724\n",
            "Best epoch: 16\n",
            "Epoch 1/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9912\n",
            "Epoch 2/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0231 - accuracy: 0.9938\n",
            "Epoch 3/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9955\n",
            "Epoch 4/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9965\n",
            "Epoch 5/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0140 - accuracy: 0.9964\n",
            "Epoch 6/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0120 - accuracy: 0.9977\n",
            "Epoch 7/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0107 - accuracy: 0.9977\n",
            "Epoch 8/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0099 - accuracy: 0.9980\n",
            "Epoch 9/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0092 - accuracy: 0.9983\n",
            "Epoch 10/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0076 - accuracy: 0.9989\n",
            "Epoch 11/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0083 - accuracy: 0.9984\n",
            "Epoch 12/19\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9987\n",
            "Epoch 13/19\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9993\n",
            "Epoch 14/19\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9993\n",
            "Epoch 15/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0050 - accuracy: 0.9992\n",
            "Epoch 16/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0066 - accuracy: 0.9988\n",
            "Epoch 17/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9995\n",
            "Epoch 18/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0031 - accuracy: 0.9998\n",
            "Epoch 19/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9991\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.9759\n",
            "Epoch 1/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.4248 - accuracy: 0.8875 - val_loss: 0.2388 - val_accuracy: 0.9305\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.2225 - accuracy: 0.9362 - val_loss: 0.1877 - val_accuracy: 0.9467\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1733 - accuracy: 0.9507 - val_loss: 0.1556 - val_accuracy: 0.9571\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1419 - accuracy: 0.9592 - val_loss: 0.1432 - val_accuracy: 0.9588\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.1202 - accuracy: 0.9652 - val_loss: 0.1317 - val_accuracy: 0.9609\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9698 - val_loss: 0.1123 - val_accuracy: 0.9681\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0927 - accuracy: 0.9733 - val_loss: 0.1095 - val_accuracy: 0.9669\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0819 - accuracy: 0.9763 - val_loss: 0.1085 - val_accuracy: 0.9677\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0741 - accuracy: 0.9784 - val_loss: 0.1035 - val_accuracy: 0.9691\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0677 - accuracy: 0.9801 - val_loss: 0.0994 - val_accuracy: 0.9716\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0611 - accuracy: 0.9822 - val_loss: 0.0998 - val_accuracy: 0.9712\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9839 - val_loss: 0.1005 - val_accuracy: 0.9706\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0512 - accuracy: 0.9860 - val_loss: 0.0957 - val_accuracy: 0.9732\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9867 - val_loss: 0.0955 - val_accuracy: 0.9730\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.0965 - val_accuracy: 0.9736\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9887 - val_loss: 0.0943 - val_accuracy: 0.9733\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0377 - accuracy: 0.9898 - val_loss: 0.0947 - val_accuracy: 0.9735\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0346 - accuracy: 0.9906 - val_loss: 0.0939 - val_accuracy: 0.9755\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0317 - accuracy: 0.9915 - val_loss: 0.1036 - val_accuracy: 0.9716\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 2s 5ms/step - loss: 0.0294 - accuracy: 0.9923 - val_loss: 0.1014 - val_accuracy: 0.9727\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0276 - accuracy: 0.9926 - val_loss: 0.0966 - val_accuracy: 0.9743\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0252 - accuracy: 0.9936 - val_loss: 0.0976 - val_accuracy: 0.9738\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0236 - accuracy: 0.9941 - val_loss: 0.0988 - val_accuracy: 0.9723\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0216 - accuracy: 0.9945 - val_loss: 0.1077 - val_accuracy: 0.9713\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0201 - accuracy: 0.9949 - val_loss: 0.1009 - val_accuracy: 0.9740\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0185 - accuracy: 0.9955 - val_loss: 0.1025 - val_accuracy: 0.9748\n",
            "Epoch 27/100\n",
            "391/391 [==============================] - 1s 4ms/step - loss: 0.0167 - accuracy: 0.9962 - val_loss: 0.1111 - val_accuracy: 0.9738\n",
            "Epoch 28/100\n",
            "391/391 [==============================] - 2s 4ms/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 0.1036 - val_accuracy: 0.9740\n",
            "Best epoch: 18\n",
            "Epoch 1/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0302 - accuracy: 0.9926\n",
            "Epoch 2/21\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0259 - accuracy: 0.9934\n",
            "Epoch 3/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0234 - accuracy: 0.9943\n",
            "Epoch 4/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0210 - accuracy: 0.9946\n",
            "Epoch 5/21\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0194 - accuracy: 0.9951\n",
            "Epoch 6/21\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.0180 - accuracy: 0.9955\n",
            "Epoch 7/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0166 - accuracy: 0.9958\n",
            "Epoch 8/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0153 - accuracy: 0.9964\n",
            "Epoch 9/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0143 - accuracy: 0.9965\n",
            "Epoch 10/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0134 - accuracy: 0.9970\n",
            "Epoch 11/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0125 - accuracy: 0.9971\n",
            "Epoch 12/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0112 - accuracy: 0.9974\n",
            "Epoch 13/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0107 - accuracy: 0.9976\n",
            "Epoch 14/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0098 - accuracy: 0.9978\n",
            "Epoch 15/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0094 - accuracy: 0.9979\n",
            "Epoch 16/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0084 - accuracy: 0.9982\n",
            "Epoch 17/21\n",
            "469/469 [==============================] - 2s 3ms/step - loss: 0.0077 - accuracy: 0.9984\n",
            "Epoch 18/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0074 - accuracy: 0.9984\n",
            "Epoch 19/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0070 - accuracy: 0.9984\n",
            "Epoch 20/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9989\n",
            "Epoch 21/21\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9989\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1207 - accuracy: 0.9763\n",
            "Epoch 1/100\n",
            "391/391 [==============================] - 2s 3ms/step - loss: 0.4315 - accuracy: 0.8851 - val_loss: 0.2412 - val_accuracy: 0.9329\n",
            "Epoch 2/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.2240 - accuracy: 0.9364 - val_loss: 0.1953 - val_accuracy: 0.9457\n",
            "Epoch 3/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1729 - accuracy: 0.9508 - val_loss: 0.1550 - val_accuracy: 0.9560\n",
            "Epoch 4/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1429 - accuracy: 0.9587 - val_loss: 0.1427 - val_accuracy: 0.9602\n",
            "Epoch 5/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1216 - accuracy: 0.9646 - val_loss: 0.1273 - val_accuracy: 0.9639\n",
            "Epoch 6/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.1052 - accuracy: 0.9700 - val_loss: 0.1198 - val_accuracy: 0.9665\n",
            "Epoch 7/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0926 - accuracy: 0.9729 - val_loss: 0.1138 - val_accuracy: 0.9683\n",
            "Epoch 8/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0825 - accuracy: 0.9764 - val_loss: 0.1063 - val_accuracy: 0.9705\n",
            "Epoch 9/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0740 - accuracy: 0.9783 - val_loss: 0.1120 - val_accuracy: 0.9689\n",
            "Epoch 10/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0676 - accuracy: 0.9805 - val_loss: 0.1003 - val_accuracy: 0.9717\n",
            "Epoch 11/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0610 - accuracy: 0.9825 - val_loss: 0.1016 - val_accuracy: 0.9710\n",
            "Epoch 12/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0557 - accuracy: 0.9839 - val_loss: 0.0991 - val_accuracy: 0.9724\n",
            "Epoch 13/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0516 - accuracy: 0.9850 - val_loss: 0.1055 - val_accuracy: 0.9718\n",
            "Epoch 14/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9870 - val_loss: 0.0974 - val_accuracy: 0.9736\n",
            "Epoch 15/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0435 - accuracy: 0.9878 - val_loss: 0.0998 - val_accuracy: 0.9720\n",
            "Epoch 16/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0398 - accuracy: 0.9890 - val_loss: 0.0963 - val_accuracy: 0.9716\n",
            "Epoch 17/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0367 - accuracy: 0.9902 - val_loss: 0.0987 - val_accuracy: 0.9726\n",
            "Epoch 18/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0337 - accuracy: 0.9913 - val_loss: 0.0971 - val_accuracy: 0.9737\n",
            "Epoch 19/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0314 - accuracy: 0.9916 - val_loss: 0.0971 - val_accuracy: 0.9735\n",
            "Epoch 20/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0285 - accuracy: 0.9926 - val_loss: 0.1048 - val_accuracy: 0.9733\n",
            "Epoch 21/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0271 - accuracy: 0.9931 - val_loss: 0.0997 - val_accuracy: 0.9741\n",
            "Epoch 22/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0251 - accuracy: 0.9934 - val_loss: 0.1074 - val_accuracy: 0.9717\n",
            "Epoch 23/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0226 - accuracy: 0.9941 - val_loss: 0.1051 - val_accuracy: 0.9728\n",
            "Epoch 24/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0212 - accuracy: 0.9944 - val_loss: 0.1074 - val_accuracy: 0.9726\n",
            "Epoch 25/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0198 - accuracy: 0.9948 - val_loss: 0.1123 - val_accuracy: 0.9729\n",
            "Epoch 26/100\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 0.1112 - val_accuracy: 0.9726\n",
            "Best epoch: 16\n",
            "Epoch 1/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0324 - accuracy: 0.9919\n",
            "Epoch 2/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0287 - accuracy: 0.9924\n",
            "Epoch 3/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0254 - accuracy: 0.9936\n",
            "Epoch 4/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0232 - accuracy: 0.9938\n",
            "Epoch 5/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0213 - accuracy: 0.9943\n",
            "Epoch 6/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0196 - accuracy: 0.9944\n",
            "Epoch 7/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0181 - accuracy: 0.9954\n",
            "Epoch 8/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0169 - accuracy: 0.9956\n",
            "Epoch 9/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0160 - accuracy: 0.9960\n",
            "Epoch 10/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0142 - accuracy: 0.9964\n",
            "Epoch 11/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0135 - accuracy: 0.9966\n",
            "Epoch 12/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0122 - accuracy: 0.9969\n",
            "Epoch 13/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 0.9972\n",
            "Epoch 14/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0107 - accuracy: 0.9975\n",
            "Epoch 15/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0098 - accuracy: 0.9979\n",
            "Epoch 16/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0092 - accuracy: 0.9980\n",
            "Epoch 17/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0086 - accuracy: 0.9980\n",
            "Epoch 18/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0078 - accuracy: 0.9983\n",
            "Epoch 19/19\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.0076 - accuracy: 0.9986\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7mYKTHc4mbT",
        "outputId": "45a6fa7e-7b03-4012-9d86-a8732f379d93"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.sequential.Sequential at 0x7fafa1d0ee90>,\n",
              " <keras.engine.sequential.Sequential at 0x7fafa1cc64d0>,\n",
              " <keras.engine.sequential.Sequential at 0x7faf8060a910>,\n",
              " <keras.engine.sequential.Sequential at 0x7faf80490090>]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If you are not worrying about underperforming, there's a shortcut you can take\n",
        "# just use the tuner to reload the top-performing models with the best weights \n",
        "# saved during the hyperparameter search, without retraining new models from\n",
        "# scratch\n",
        "\n",
        "best_modelsv2 = tuner.get_best_models(top_n)\n"
      ],
      "metadata": {
        "id": "Rjkf07PguxAc"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the performance of both models on the accuracy of the test data\n",
        "# case 1: best_models - retrain on full data\n",
        "# case 2: best_models picked from top_n models after hyperparameter tuning with KT"
      ],
      "metadata": {
        "id": "PdG_A4PIvFyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for model in best_models:\n",
        "  model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5nqmO8c5Zbc",
        "outputId": "1e7b6017-c285-4393-9a90-2d0381c1c99b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1051 - accuracy: 0.9757\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1074 - accuracy: 0.9759\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1207 - accuracy: 0.9763\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# smaller loss, these models are not trained on the full data\n",
        "for model in best_modelsv2:\n",
        "  model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r-3Ui4U5ce5",
        "outputId": "ce13bee2-6db1-41d1-c895-972d0aea7d3f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0842 - accuracy: 0.9782\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0887 - accuracy: 0.9737\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0941 - accuracy: 0.9744\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0930 - accuracy: 0.9752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# view the summary of one of the bestmodels\n",
        "best_model = best_models[2]\n",
        "\n",
        "best_model.build(input_shape=(None,28,28))\n",
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYHcKmUe6LsU",
        "outputId": "a85b507a-4856-4343-8a97-85cec7f6da90"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 64)                50240     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 50,890\n",
            "Trainable params: 50,890\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print a summary of the search results\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PKiMzcl6sMk",
        "outputId": "0cadb670-3294-4b7d-9a9b-9b5b6cf0e75c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in mnist_kt_test/untitled_project\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7fafa01d8110>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9754500091075897\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: adam\n",
            "Score: 0.9751999974250793\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.974700003862381\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9745000004768372\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9745000004768372\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9743499755859375\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9739000201225281\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: rmsprop\n",
            "Score: 0.9733499884605408\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 64\n",
            "optimizer: adam\n",
            "Score: 0.9731000065803528\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "units: 32\n",
            "optimizer: adam\n",
            "Score: 0.9704000055789948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[:2]\n",
        "y_conv = keras.utils.to_categorical(y_train, 10)\n",
        "y_conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0emPnxgj658U",
        "outputId": "e3d82158-ee68-411d-c138-85a9714ad9fb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With the above representation if we convert y values to categorical\n",
        "# in the compile method, then loss = 'categorical_crossentropy'\n",
        "# but if we use the sparse representions, 1, 2, 3,4, \n",
        "# then loss= 'sparse_categorical_crossentropy'\n"
      ],
      "metadata": {
        "id": "dSJVBiI47MnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}