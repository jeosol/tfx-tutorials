{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc666bf9",
   "metadata": {},
   "source": [
    "# Data validation using TFX Pipeline and TensorFlow Data Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e213fa9",
   "metadata": {},
   "source": [
    "The first task in any data science or ML project is to understand and clean the data, which includes:\n",
    "\n",
    "* Understanding the data types, distributions, and other information (e.g., mean value, or number of uniques) about each feature\n",
    "\n",
    "* Generating a preliminary schema that describes the data\n",
    "\n",
    "* Identifying anomalies and missing values in the data with respect to given schema\n",
    "\n",
    "In this tutorial, we will create two TFX pipelines.\n",
    "\n",
    "First, we will create a pipeline to analyze the dataset and generate a preliminary schema of the given dataset. This pipeline will include two new components, StatisticsGen and SchemaGen.\n",
    "\n",
    "Once we have a proper schema of the data, we will create a pipeline to train an ML classification model based on the pipeline from the previous tutorial. In this pipeline, we will use the schema from the first pipeline and a new component, ExampleValidator, to validate the input data.\n",
    "\n",
    "The three new components, StatisticsGen, SchemaGen and ExampleValidator, are TFX components for data analysis and validation, and they are implemented using the TensorFlow Data Validation library.\n",
    "\n",
    "Please see Understanding TFX Pipelines to learn more about various concepts in TFX.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.1\n",
      "TFX version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79606d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We will create two pipelines. One for schema generation and one for training.\n",
    "SCHEMA_PIPELINE_NAME = \"penguin-tfdv-schema\"\n",
    "PIPELINE_NAME = \"penguin-tfdv\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "SCHEMA_PIPELINE_ROOT = os.path.join('pipelines', SCHEMA_PIPELINE_NAME)\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "SCHEMA_METADATA_PATH = os.path.join('metadata', SCHEMA_PIPELINE_NAME,\n",
    "                                    'metadata.db')\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a815d20d",
   "metadata": {},
   "source": [
    "## Prepare example data\n",
    "\n",
    "\n",
    "We will download the example dataset for use in our TFX pipeline. The dataset we are using is Palmer Penguins dataset which is also used in other TFX examples.\n",
    "\n",
    "There are four numeric features in this dataset:\n",
    "\n",
    "culmen_length_mm\n",
    "\n",
    "culmen_depth_mm\n",
    "\n",
    "flipper_length_mm\n",
    "\n",
    "body_mass_g\n",
    "\n",
    "All features were already normalized to have range [0,1]. We will build a classification model which predicts the species of penguins.\n",
    "\n",
    "Because the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc2bd8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/tfx-datadrkbvfrb/data.csv', <http.client.HTTPMessage at 0x7f276141d220>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "_data_url = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/labelled/penguins_processed.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_url, _data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3f0a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g\r\n",
      "0,0.2545454545454545,0.6666666666666666,0.15254237288135594,0.2916666666666667\r\n",
      "0,0.26909090909090905,0.5119047619047618,0.23728813559322035,0.3055555555555556\r\n",
      "0,0.29818181818181805,0.5833333333333334,0.3898305084745763,0.1527777777777778\r\n",
      "0,0.16727272727272732,0.7380952380952381,0.3559322033898305,0.20833333333333334\r\n",
      "0,0.26181818181818167,0.892857142857143,0.3050847457627119,0.2638888888888889\r\n",
      "0,0.24727272727272717,0.5595238095238096,0.15254237288135594,0.2569444444444444\r\n",
      "0,0.25818181818181823,0.773809523809524,0.3898305084745763,0.5486111111111112\r\n",
      "0,0.32727272727272727,0.5357142857142859,0.1694915254237288,0.1388888888888889\r\n",
      "0,0.23636363636363636,0.9642857142857142,0.3220338983050847,0.3055555555555556\r\n"
     ]
    }
   ],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2544fe9d",
   "metadata": {},
   "source": [
    "You should be able to see five feature columns. species is one of 0, 1 or 2, and all other features should have values between 0 and 1. We will create a TFX pipeline to analyze this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b800e63",
   "metadata": {},
   "source": [
    "## Generate a preliminary schema\n",
    "TFX pipelines are defined using Python APIs. We will create a pipeline to generate a schema from the input examples automatically. This schema can be reviewed by a human and adjusted as needed. Once the schema is finalized it can be used for training and example validation in later tasks.\n",
    "\n",
    "In addition to CsvExampleGen which is used in Simple TFX Pipeline Tutorial, we will use StatisticsGen and SchemaGen:\n",
    "\n",
    "* StatisticsGen calculates statistics for the dataset.\n",
    "* SchemaGen examines the statistics and creates an initial data schema.\n",
    "See the guides for each component or TFX components tutorial to learn more on these components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d152c6b",
   "metadata": {},
   "source": [
    "### Write a pipeline definition\n",
    "\n",
    "We define a function to create a TFX pipeline. A Pipeline object represents a TFX pipeline which can be run using one of pipeline orchestration systems that TFX supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8aa62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_schema_pipeline(pipeline_name: str,\n",
    "                            pipeline_root: str,\n",
    "                            data_root: str,\n",
    "                            metadata_path: str) -> tfx.dsl.Pipeline:\n",
    "  \"\"\"Creates a pipeline for schema generation.\"\"\"\n",
    "  # Brings data into the pipeline.\n",
    "  example_gen = tfx.components.CsvExampleGen(input_base=data_root)\n",
    "\n",
    "  # NEW: Computes statistics over data for visualization and schema generation.\n",
    "  statistics_gen = tfx.components.StatisticsGen(\n",
    "      examples=example_gen.outputs['examples'])\n",
    "\n",
    "  # NEW: Generates schema based on the generated statistics.\n",
    "  schema_gen = tfx.components.SchemaGen(\n",
    "      statistics=statistics_gen.outputs['statistics'], infer_feature_shape=True)\n",
    "\n",
    "  components = [\n",
    "      example_gen,\n",
    "      statistics_gen,\n",
    "      schema_gen,\n",
    "  ]\n",
    "\n",
    "  return tfx.dsl.Pipeline(\n",
    "      pipeline_name=pipeline_name,\n",
    "      pipeline_root=pipeline_root,\n",
    "      metadata_connection_config=tfx.orchestration.metadata\n",
    "      .sqlite_metadata_connection_config(metadata_path),\n",
    "      components=components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9f41be",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "We will use LocalDagRunner as in the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f83a7b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"SchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"metadata/penguin-tfdv-schema/metadata.db\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"metadata/penguin-tfdv-schema/metadata.db\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "INFO:absl:Component CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"/tmp/tfx-datadrkbvfrb\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 1\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1653629340,sum_checksum:1653629340\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 1,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'input_base': '/tmp/tfx-datadrkbvfrb', 'output_file_format': 5, 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:25648,xor_checksum:1653629340,sum_checksum:1653629340'}, execution_output_uri='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/executor_execution/1/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/stateful_working_dir/2022-05-27T00:29:11.287512', tmp_dir='pipelines/penguin-tfdv-schema/CsvExampleGen/.system/executor_execution/1/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"/tmp/tfx-datadrkbvfrb\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 1,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfdv-schema\"\n",
      ", pipeline_run_id='2022-05-27T00:29:11.287512')\n",
      "INFO:absl:Generating examples.\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Processing input csv data /tmp/tfx-datadrkbvfrb/* to TFExample.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "WARNING:apache_beam.io.tfrecordio:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 1 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1653629340,sum_checksum:1653629340\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.1\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 1\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component CsvExampleGen is finished.\n",
      "INFO:absl:Component StatisticsGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-05-27T00:29:11.287512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 2\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=2, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"pipelines/penguin-tfdv-schema/CsvExampleGen/examples/1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:25648,xor_checksum:1653629340,sum_checksum:1653629340\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:CsvExampleGen:examples:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.1\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1653629354471\n",
      "last_update_time_since_epoch: 1653629354471\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv-schema/StatisticsGen/.system/executor_execution/2/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/StatisticsGen/.system/stateful_working_dir/2022-05-27T00:29:11.287512', tmp_dir='pipelines/penguin-tfdv-schema/StatisticsGen/.system/executor_execution/2/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-05-27T00:29:11.287512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfdv-schema\"\n",
      ", pipeline_run_id='2022-05-27T00:29:11.287512')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2/Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2/Split-eval.\n",
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 2 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.1\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 2\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component StatisticsGen is finished.\n",
      "INFO:absl:Component SchemaGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-05-27T00:29:11.287512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Going to run a new execution 3\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'statistics': [Artifact(artifact: id: 2\n",
      "type_id: 17\n",
      "uri: \"pipelines/penguin-tfdv-schema/StatisticsGen/statistics/2\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:StatisticsGen:statistics:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.1\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1653629357825\n",
      "last_update_time_since_epoch: 1653629357825\n",
      ", artifact_type: id: 17\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/SchemaGen/schema/3\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'infer_feature_shape': 1, 'exclude_splits': '[]'}, execution_output_uri='pipelines/penguin-tfdv-schema/SchemaGen/.system/executor_execution/3/executor_output.pb', stateful_working_dir='pipelines/penguin-tfdv-schema/SchemaGen/.system/stateful_working_dir/2022-05-27T00:29:11.287512', tmp_dir='pipelines/penguin-tfdv-schema/SchemaGen/.system/executor_execution/3/.temp/', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"2022-05-27T00:29:11.287512\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"penguin-tfdv-schema.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"2022-05-27T00:29:11.287512\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"penguin-tfdv-schema.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"penguin-tfdv-schema\"\n",
      ", pipeline_run_id='2022-05-27T00:29:11.287512')\n",
      "INFO:absl:Processing schema from statistics for split train.\n",
      "INFO:absl:Processing schema from statistics for split eval.\n",
      "INFO:absl:Schema written to pipelines/penguin-tfdv-schema/SchemaGen/schema/3/schema.pbtxt.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 3 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"pipelines/penguin-tfdv-schema/SchemaGen/schema/3\"\n",
      "custom_properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"penguin-tfdv-schema:2022-05-27T00:29:11.287512:SchemaGen:schema:0\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.7.1\"\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}) for execution 3\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "INFO:absl:Component SchemaGen is finished.\n"
     ]
    }
   ],
   "source": [
    "tfx.orchestration.LocalDagRunner().run(\n",
    "  _create_schema_pipeline(\n",
    "      pipeline_name=SCHEMA_PIPELINE_NAME,\n",
    "      pipeline_root=SCHEMA_PIPELINE_ROOT,\n",
    "      data_root=DATA_ROOT,\n",
    "      metadata_path=SCHEMA_METADATA_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6275667e",
   "metadata": {},
   "source": [
    "You should see \"INFO:absl:Component SchemaGen is finished.\" if the pipeline finished successfully.\n",
    "\n",
    "We will examine the output of the pipeline to understand our dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e573c",
   "metadata": {},
   "source": [
    "### Review outputs of the pipeline\n",
    "\n",
    "As explained in the previous tutorial, a TFX pipeline produces two kinds of outputs, artifacts and a metadata DB(MLMD) which contains metadata of artifacts and pipeline executions. We defined the location of these outputs in the above cells. By default, artifacts are stored under the pipelines directory and metadata is stored as a sqlite database under the metadata directory.\n",
    "\n",
    "You can use MLMD APIs to locate these outputs programatically. First, we will define some utility functions to search for the output artifacts that were just produced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74201f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_metadata.proto import metadata_store_pb2\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.portable.mlmd import execution_lib\n",
    "\n",
    "# TODO(b/171447278): Move these functions into the TFX library.\n",
    "\n",
    "def get_latest_artifacts(metadata, pipeline_name, component_id):\n",
    "  \"\"\"Output artifacts of the latest run of the component.\"\"\"\n",
    "  context = metadata.store.get_context_by_type_and_name(\n",
    "      'node', f'{pipeline_name}.{component_id}')\n",
    "  executions = metadata.store.get_executions_by_context(context.id)\n",
    "  latest_execution = max(executions,\n",
    "                         key=lambda e:e.last_update_time_since_epoch)\n",
    "  return execution_lib.get_artifacts_dict(metadata, latest_execution.id,\n",
    "                                          [metadata_store_pb2.Event.OUTPUT])\n",
    "\n",
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.experimental.interactive import visualizations\n",
    "\n",
    "def visualize_artifacts(artifacts):\n",
    "  \"\"\"Visualizes artifacts using standard visualization modules.\"\"\"\n",
    "  for artifact in artifacts:\n",
    "    visualization = visualizations.get_registry().get_visualization(\n",
    "        artifact.type_name)\n",
    "    if visualization:\n",
    "      visualization.display(artifact)\n",
    "\n",
    "from tfx.orchestration.experimental.interactive import standard_visualizations\n",
    "standard_visualizations.register_standard_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c1d4d",
   "metadata": {},
   "source": [
    "Now we can examine the outputs from the pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f3c4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:MetadataStore with DB connection initialized\n"
     ]
    }
   ],
   "source": [
    "# Non-public APIs, just for showcase.\n",
    "from tfx.orchestration.metadata import Metadata\n",
    "from tfx.types import standard_component_specs\n",
    "\n",
    "metadata_connection_config = tfx.orchestration.metadata.sqlite_metadata_connection_config(\n",
    "    SCHEMA_METADATA_PATH)\n",
    "\n",
    "with Metadata(metadata_connection_config) as metadata_handler:\n",
    "  # Find output artifacts from MLMD.\n",
    "  stat_gen_output = get_latest_artifacts(metadata_handler, SCHEMA_PIPELINE_NAME,\n",
    "                                         'StatisticsGen')\n",
    "  stats_artifacts = stat_gen_output[standard_component_specs.STATISTICS_KEY]\n",
    "\n",
    "  schema_gen_output = get_latest_artifacts(metadata_handler,\n",
    "                                           SCHEMA_PIPELINE_NAME, 'SchemaGen')\n",
    "  schema_artifacts = schema_gen_output[standard_component_specs.SCHEMA_KEY]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294630b",
   "metadata": {},
   "source": [
    "It is time to examine the outputs from each component. As described above, Tensorflow Data Validation(TFDV) is used in StatisticsGen and SchemaGen, and TFDV also provides visualization of the outputs from these components.\n",
    "\n",
    "In this tutorial, we will use the visualization helper methods in TFX which use TFDV internally to show the visualization.\n",
    "\n",
    "### Examine the output from StatisticsGen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac2e8ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><b>'train' split:</b></div><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>'eval' split:</b></div><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# docs-infra: no-execute\n",
    "visualize_artifacts(stats_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b3280",
   "metadata": {},
   "source": [
    "You can see various stats for the input data. These statistics are supplied to SchemaGen to construct an initial schema of data automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c91a54d",
   "metadata": {},
   "source": [
    "### Examine the output from SchemaGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66b27b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'body_mass_g'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'culmen_depth_mm'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'culmen_length_mm'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'flipper_length_mm'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'species'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Type  Presence Valency Domain\n",
       "Feature name                                       \n",
       "'body_mass_g'        FLOAT  required              -\n",
       "'culmen_depth_mm'    FLOAT  required              -\n",
       "'culmen_length_mm'   FLOAT  required              -\n",
       "'flipper_length_mm'  FLOAT  required              -\n",
       "'species'              INT  required              -"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_artifacts(schema_artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dca877",
   "metadata": {},
   "source": [
    "This schema is automatically inferred from the output of StatisticsGen. You should be able to see 4 FLOAT features and 1 INT feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d8a29e",
   "metadata": {},
   "source": [
    "### Export the schema for future use\n",
    "\n",
    "\n",
    "We need to review and refine the generated schema. The reviewed schema needs to be persisted to be used in subsequent pipelines for ML model training. In other words, you might want to add the schema file to your version control system for actual use cases. In this tutorial, we will just copy the schema to a predefined filesystem path for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8277c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'schema/schema.pbtxt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "SCHEMA_PATH = 'schema'\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "_generated_path = os.path.join(schema_artifacts[0].uri, _schema_filename)\n",
    "\n",
    "# Copy the 'schema.pbtxt' file from the artifact uri to a predefined path.\n",
    "shutil.copy(_generated_path, SCHEMA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a147c8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema at schema-----\n",
      "feature {\r\n",
      "  name: \"body_mass_g\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"culmen_depth_mm\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"culmen_length_mm\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"flipper_length_mm\"\r\n",
      "  type: FLOAT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n",
      "feature {\r\n",
      "  name: \"species\"\r\n",
      "  type: INT\r\n",
      "  presence {\r\n",
      "    min_fraction: 1.0\r\n",
      "    min_count: 1\r\n",
      "  }\r\n",
      "  shape {\r\n",
      "    dim {\r\n",
      "      size: 1\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "print(f'Schema at {SCHEMA_PATH}-----')\n",
    "!cat {SCHEMA_PATH}/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075be12",
   "metadata": {},
   "source": [
    "You should be sure to review and possibly edit the schema definition as needed. In this tutorial, we will just use the generated schema unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5fced2",
   "metadata": {},
   "source": [
    "## Validate input examples and train an ML model\n",
    "\n",
    "We will go back to the pipeline that we created in Simple TFX Pipeline Tutorial, to train an ML model and use the generated schema for writing the model training code.\n",
    "\n",
    "We will also add an ExampleValidator component which will look for anomalies and missing values in the incoming dataset with respect to the schema.\n",
    "\n",
    "## Write model training code\n",
    "\n",
    "We need to write the model code as we did in Simple TFX Pipeline Tutorial.\n",
    "\n",
    "The model itself is the same as in the previous tutorial, but this time we will use the schema generated from the previous pipeline instead of specifying features manually. Most of the code was not changed. The only difference is that we do not need to specify the names and types of features in this file. Instead, we read them from the schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fe32b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
