{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfe56b9",
   "metadata": {},
   "source": [
    "## Feature Engineering using TFX Pipeline and TensorFlow Transform\n",
    "\n",
    "Transform input data and train a model with a TFX pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdd83d",
   "metadata": {},
   "source": [
    "You can increase the predictive quality of your data and/or reduce dimensionality with feature engineering. One of the benefits of using TFX is that you will write your transformation code once, and the resulting transforms will be consistent between training and serving in order to avoid training/serving skew.\n",
    "\n",
    "We will add a Transform component to the pipeline. The Transform component is implemented using the tf.transform library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a630ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.1\n",
      "TFX version: 1.7.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow version: {}'.format(tf.__version__))\n",
    "from tfx import v1 as tfx\n",
    "print('TFX version: {}'.format(tfx.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0874efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PIPELINE_NAME = \"penguin-transform\"\n",
    "\n",
    "# Output directory to store artifacts generated from the pipeline.\n",
    "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
    "# Path to a SQLite DB file to use as an MLMD storage.\n",
    "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
    "# Output directory where created models from the pipeline will be exported.\n",
    "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
    "\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)  # Set default logging level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab26d7c",
   "metadata": {},
   "source": [
    "### Prepare example data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123274f",
   "metadata": {},
   "source": [
    "We will download the example dataset for use in our TFX pipeline. The dataset we are using is Palmer Penguins dataset.\n",
    "\n",
    "However, unlike previous tutorials which used an already preprocessed dataset, we will use the **raw** Palmer Penguins dataset.\n",
    "\n",
    "Because the TFX ExampleGen component reads inputs from a directory, we need to create a directory and copy the dataset to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2d146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/tmp/tfx-datalvyu4pva/data.csv', <http.client.HTTPMessage at 0x7fbfba1883a0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "import tempfile\n",
    "\n",
    "DATA_ROOT = tempfile.mkdtemp(prefix='tfx-data')  # Create a temporary directory.\n",
    "_data_path = 'https://storage.googleapis.com/download.tensorflow.org/data/palmer_penguins/penguins_size.csv'\n",
    "_data_filepath = os.path.join(DATA_ROOT, \"data.csv\")\n",
    "urllib.request.urlretrieve(_data_path, _data_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d51e1",
   "metadata": {},
   "source": [
    "Take a quick look at what the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f070eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\r\n",
      "Adelie,Torgersen,39.1,18.7,181,3750,MALE\r\n",
      "Adelie,Torgersen,39.5,17.4,186,3800,FEMALE\r\n",
      "Adelie,Torgersen,40.3,18,195,3250,FEMALE\r\n",
      "Adelie,Torgersen,NA,NA,NA,NA,NA\r\n",
      "Adelie,Torgersen,36.7,19.3,193,3450,FEMALE\r\n",
      "Adelie,Torgersen,39.3,20.6,190,3650,MALE\r\n",
      "Adelie,Torgersen,38.9,17.8,181,3625,FEMALE\r\n",
      "Adelie,Torgersen,39.2,19.6,195,4675,MALE\r\n",
      "Adelie,Torgersen,34.1,18.1,193,3475,NA\r\n"
     ]
    }
   ],
   "source": [
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5467a9",
   "metadata": {},
   "source": [
    "There are some entries with missing values which are represented as NA. We will just delete those entries in this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04532e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "species,island,culmen_length_mm,culmen_depth_mm,flipper_length_mm,body_mass_g,sex\r\n",
      "Adelie,Torgersen,39.1,18.7,181,3750,MALE\r\n",
      "Adelie,Torgersen,39.5,17.4,186,3800,FEMALE\r\n",
      "Adelie,Torgersen,40.3,18,195,3250,FEMALE\r\n",
      "Adelie,Torgersen,36.7,19.3,193,3450,FEMALE\r\n",
      "Adelie,Torgersen,39.3,20.6,190,3650,MALE\r\n",
      "Adelie,Torgersen,38.9,17.8,181,3625,FEMALE\r\n",
      "Adelie,Torgersen,39.2,19.6,195,4675,MALE\r\n",
      "Adelie,Torgersen,41.1,17.6,182,3200,FEMALE\r\n",
      "Adelie,Torgersen,38.6,21.2,191,3800,MALE\r\n"
     ]
    }
   ],
   "source": [
    "!sed -i '/\\bNA\\b/d' {_data_filepath}\n",
    "!head {_data_filepath}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d649bae",
   "metadata": {},
   "source": [
    "You should be able to see seven features which describe penguins. We will use the same set of features as the previous tutorials - 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g' - and will predict the 'species' of a penguin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90e070",
   "metadata": {},
   "source": [
    "The only difference will be that the input data is not preprocessed. Note that we will not use other features like 'island' or 'sex' in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad4ed56",
   "metadata": {},
   "source": [
    "### Prepare a schema file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0024cf",
   "metadata": {},
   "source": [
    "As described in Data validation using TFX Pipeline and TensorFlow Data Validation Tutorial, we need a schema file for the dataset. Because the dataset is different from the previous tutorial we need to generate it again. In this tutorial, we will skip those steps and just use a prepared schema file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c307542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('schema/schema.pbtxt', <http.client.HTTPMessage at 0x7fbfba188ac0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "SCHEMA_PATH = 'schema'\n",
    "\n",
    "_schema_uri = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/schema/raw/schema.pbtxt'\n",
    "_schema_filename = 'schema.pbtxt'\n",
    "_schema_filepath = os.path.join(SCHEMA_PATH, _schema_filename)\n",
    "\n",
    "os.makedirs(SCHEMA_PATH, exist_ok=True)\n",
    "urllib.request.urlretrieve(_schema_uri, _schema_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab21d26",
   "metadata": {},
   "source": [
    "This schema file was created with the same pipeline as in the previous tutorial without any manual changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0d8c5f",
   "metadata": {},
   "source": [
    "## Create a pipeline\n",
    "TFX pipelines are defined using Python APIs. We will add Transform component to the pipeline we created in the Data Validation tutorial.\n",
    "\n",
    "A Transform component requires input data from an ExampleGen component and a schema from a SchemaGen component, and produces a \"transform graph\". The output will be used in a Trainer component. Transform can optionally produce \"transformed data\" in addition, which is the materialized data after transformation. However, we will transform data during training in this tutorial without materialization of the intermediate transformed data.\n",
    "\n",
    "One thing to note is that we need to define a Python function, preprocessing_fn to describe how input data should be transformed. This is similar to a Trainer component which also requires user code for model definition.\n",
    "\n",
    "### Write preprocessing and training code\n",
    "We need to define two Python functions. One for Transform and one for Trainer.\n",
    "\n",
    "**preprocessing_fn**\n",
    "\n",
    "The Transform component will find a function named preprocessing_fn in the given module file as we did for Trainer component. You can also specify a specific function using the preprocessing_fn parameter of the Transform component.\n",
    "\n",
    "In this example, we will do two kinds of transformation. For continuous numeric features like culmen_length_mm and body_mass_g, we will normalize these values using the tft.scale_to_z_score function. For the label feature, we need to convert string labels into numeric index values. We will use tf.lookup.StaticHashTable for conversion.\n",
    "\n",
    "To identify transformed fields easily, we append a _xf suffix to the transformed feature names.\n",
    "\n",
    "**run_fn**\n",
    "\n",
    "The model itself is almost the same as in the previous tutorials, but this time we will transform the input data using the transform graph from the Transform component.\n",
    "\n",
    "One more important difference compared to the previous tutorial is that now we export a model for serving which includes not only the computation graph of the model, but also the transform graph for preprocessing, which is generated in Transform component. We need to define a separate function which will be used for serving incoming requests. You can see that the same function _apply_preprocessing was used for both of the training data and the serving request.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32874195",
   "metadata": {},
   "outputs": [],
   "source": [
    "_module_file = 'penguin_utils.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3fb279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing penguin_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_module_file}\n",
    "\n",
    "\n",
    "from typing import List, Text\n",
    "from absl import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "from tfx import v1 as tfx\n",
    "from tfx_bsl.public import tfxio\n",
    "\n",
    "# Specify features that we will use.\n",
    "_FEATURE_KEYS = [\n",
    "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
    "]\n",
    "_LABEL_KEY = 'species'\n",
    "\n",
    "_TRAIN_BATCH_SIZE = 20\n",
    "_EVAL_BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "# NEW: TFX Transform will call this function.\n",
    "def preprocessing_fn(inputs):\n",
    "  \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "\n",
    "  Args:\n",
    "    inputs: map from feature keys to raw not-yet-transformed features.\n",
    "\n",
    "  Returns:\n",
    "    Map from string feature key to transformed feature.\n",
    "  \"\"\"\n",
    "  outputs = {}\n",
    "\n",
    "  # Uses features defined in _FEATURE_KEYS only.\n",
    "  for key in _FEATURE_KEYS:\n",
    "    # tft.scale_to_z_score computes the mean and variance of the given feature\n",
    "    # and scales the output based on the result.\n",
    "    outputs[key] = tft.scale_to_z_score(inputs[key])\n",
    "\n",
    "  # For the label column we provide the mapping from string to index.\n",
    "  # We could instead use `tft.compute_and_apply_vocabulary()` in order to\n",
    "  # compute the vocabulary dynamically and perform a lookup.\n",
    "  # Since in this example there are only 3 possible values, we use a hard-coded\n",
    "  # table for simplicity.\n",
    "  table_keys = ['Adelie', 'Chinstrap', 'Gentoo']\n",
    "  initializer = tf.lookup.KeyValueTensorInitializer(\n",
    "      keys=table_keys,\n",
    "      values=tf.cast(tf.range(len(table_keys)), tf.int64),\n",
    "      key_dtype=tf.string,\n",
    "      value_dtype=tf.int64)\n",
    "  table = tf.lookup.StaticHashTable(initializer, default_value=-1)\n",
    "  outputs[_LABEL_KEY] = table.lookup(inputs[_LABEL_KEY])\n",
    "\n",
    "  return outputs\n",
    "\n",
    "\n",
    "# NEW: This function will apply the same transform operation to training data\n",
    "#      and serving requests.\n",
    "def _apply_preprocessing(raw_features, tft_layer):\n",
    "  transformed_features = tft_layer(raw_features)\n",
    "  if _LABEL_KEY in raw_features:\n",
    "    transformed_label = transformed_features.pop(_LABEL_KEY)\n",
    "    return transformed_features, transformed_label\n",
    "  else:\n",
    "    return transformed_features, None\n",
    "\n",
    "\n",
    "# NEW: This function will create a handler function which gets a serialized\n",
    "#      tf.example, preprocess and run an inference with it.\n",
    "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
    "  # We must save the tft_layer to the model to ensure its assets are kept and\n",
    "  # tracked.\n",
    "  model.tft_layer = tf_transform_output.transform_features_layer()\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')\n",
    "  ])\n",
    "  def serve_tf_examples_fn(serialized_tf_examples):\n",
    "    # Expected input is a string which is serialized tf.Example format.\n",
    "    feature_spec = tf_transform_output.raw_feature_spec()\n",
    "    # Because input schema includes unnecessary fields like 'species' and\n",
    "    # 'island', we filter feature_spec to include required keys only.\n",
    "    required_feature_spec = {\n",
    "        k: v for k, v in feature_spec.items() if k in _FEATURE_KEYS\n",
    "    }\n",
    "    parsed_features = tf.io.parse_example(serialized_tf_examples,\n",
    "                                          required_feature_spec)\n",
    "\n",
    "    # Preprocess parsed input with transform operation defined in\n",
    "    # preprocessing_fn().\n",
    "    transformed_features, _ = _apply_preprocessing(parsed_features,\n",
    "                                                   model.tft_layer)\n",
    "    # Run inference with ML model.\n",
    "    return model(transformed_features)\n",
    "\n",
    "  return serve_tf_examples_fn\n",
    "\n",
    "\n",
    "def _input_fn(file_pattern: List[Text],\n",
    "              data_accessor: tfx.components.DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  dataset = data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      tfxio.TensorFlowDatasetOptions(batch_size=batch_size),\n",
    "      schema=tf_transform_output.raw_metadata.schema)\n",
    "\n",
    "  transform_layer = tf_transform_output.transform_features_layer()\n",
    "  def apply_transform(raw_features):\n",
    "    return _apply_preprocessing(raw_features, transform_layer)\n",
    "\n",
    "  return dataset.map(apply_transform).repeat()\n",
    "\n",
    "\n",
    "def _build_keras_model() -> tf.keras.Model:\n",
    "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
    "\n",
    "  Returns:\n",
    "    A Keras Model.\n",
    "  \"\"\"\n",
    "  # The model below is built with Functional API, please refer to\n",
    "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
    "  inputs = [\n",
    "      keras.layers.Input(shape=(1,), name=key)\n",
    "      for key in _FEATURE_KEYS\n",
    "  ]\n",
    "  d = keras.layers.concatenate(inputs)\n",
    "  for _ in range(2):\n",
    "    d = keras.layers.Dense(8, activation='relu')(d)\n",
    "  outputs = keras.layers.Dense(3)(d)\n",
    "\n",
    "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "  model.compile(\n",
    "      optimizer=keras.optimizers.Adam(1e-2),\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "  model.summary(print_fn=logging.info)\n",
    "  return model\n",
    "\n",
    "\n",
    "# TFX Trainer will call this function.\n",
    "def run_fn(fn_args: tfx.components.FnArgs):\n",
    "  \"\"\"Train the model based on given args.\n",
    "\n",
    "  Args:\n",
    "    fn_args: Holds args used to train the model as name/value pairs.\n",
    "  \"\"\"\n",
    "  tf_transform_output = tft.TFTransformOutput(fn_args.transform_output)\n",
    "\n",
    "  train_dataset = _input_fn(\n",
    "      fn_args.train_files,\n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output,\n",
    "      batch_size=_TRAIN_BATCH_SIZE)\n",
    "  eval_dataset = _input_fn(\n",
    "      fn_args.eval_files,\n",
    "      fn_args.data_accessor,\n",
    "      tf_transform_output,\n",
    "      batch_size=_EVAL_BATCH_SIZE)\n",
    "\n",
    "  model = _build_keras_model()\n",
    "  model.fit(\n",
    "      train_dataset,\n",
    "      steps_per_epoch=fn_args.train_steps,\n",
    "      validation_data=eval_dataset,\n",
    "      validation_steps=fn_args.eval_steps)\n",
    "\n",
    "  # NEW: Save a computation graph including transform layer.\n",
    "  signatures = {\n",
    "      'serving_default': _get_serve_tf_examples_fn(model, tf_transform_output),\n",
    "  }\n",
    "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7eaeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfx",
   "language": "python",
   "name": "tfx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
